/*
    Appellation: multi-head <module>
    Contrib: @FL03
*/

/// Multi-Headed attention is the first evolution of the Scaled Dot-Product Attention
/// mechanism. They allow the model to jointly attend to information from different
/// representation subspaces at different positions.
pub struct MultiHeadAttention;
